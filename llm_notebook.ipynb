{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ DeepSeek Model Versions & Recommendations\n",
    "\n",
    "DeepSeek offers various model sizes optimized for different use cases. Below is a comparison of the available versions to help you choose the best one for your needs.\n",
    "\n",
    "## âœ… Recommended Lightweight DeepSeek Versions\n",
    "\n",
    "| **Model**                 | **Command**                          | **Size**  | **Best For** |\n",
    "|---------------------------|------------------------------------|-----------|-------------|\n",
    "| **DeepSeek 1.5B (default)** | `ollama pull deepseek-r1:1.5b` | **1.5B params (~2GB RAM needed)** | Lightest version, fastest |\n",
    "| **DeepSeek 7B**            | `ollama pull deepseek-r1:7b` | **7B params (~10GB RAM needed)** | Balanced speed & power |\n",
    "| **DeepSeek 8B (Llama)**    | `ollama pull deepseek-r1:8b` | **8B params (~12GB RAM needed)** | Llama3-based, better reasoning |\n",
    "| **DeepSeek 14B**           | `ollama pull deepseek-r1:14b` | **14B params (~20GB RAM needed)** | More accurate but heavier |\n",
    "\n",
    "## My Choice: \n",
    "âœ… `deepseek-r1:1.5b`    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Overview**\n",
    "This pipeline allows me to:\n",
    "- **Extract text from PDFs**\n",
    "- **Chunk text into smaller sections**\n",
    "- **Embed text using embeddings**\n",
    "- **Store and retrieve embeddings using FAISS**\n",
    "- **Query the system using DeepSeek LLM via Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text if text.strip() else None  # Avoid empty documents\n",
    "\n",
    "def load_and_process_pdfs(pdf_files):\n",
    "    \"\"\"Loads multiple PDFs and extracts valid text.\"\"\"\n",
    "    documents = [extract_text_from_pdf(pdf) for pdf in pdf_files if extract_text_from_pdf(pdf)]\n",
    "    return [doc for doc in documents if isinstance(doc, str) and doc.strip()]  \n",
    "\n",
    "def split_text_into_chunks(documents, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Splits text into smaller chunks for efficient processing.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = []\n",
    "    for doc in documents:\n",
    "        texts.extend(text_splitter.split_text(doc))\n",
    "    return texts\n",
    "\n",
    "def create_faiss_vectorstore(texts, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Generates embeddings and stores them in a FAISS vector database.\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    documents = [Document(page_content=text) for text in texts]  \n",
    "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"âœ… FAISS vector store successfully created!\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(pdf_files, query):\n",
    "    llm = Ollama(model=\"deepseek-r1:1.5b\") \n",
    "    documents = load_and_process_pdfs(pdf_files) \n",
    "    texts = split_text_into_chunks(documents)  \n",
    "    vectorstore = create_faiss_vectorstore(texts)  \n",
    "    \n",
    "    retriever = vectorstore.as_retriever()\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
    "    response = qa_chain.run(query)\n",
    "    clean_response = response.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
    "    return \"\\n\".join(clean_response.split(\". \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS vector store successfully created!\n",
      "Okay, so I'm trying to answer this question about machine learning\n",
      "The user has given me several books and some context from them\n",
      "Let me break it down step by step.\n",
      "\n",
      "First, they mentioned Stuart Russell and Peter Norvig's \"Artificial Intelligence: A Modern Approach\" which is a big book covering ML topics\n",
      "It includes things like algorithms and how they work, the bias-variance tradeoff, etc\n",
      "Then there's a chapter on ethics in computing from Berler and Brunnstein, 2001\n",
      "They talk about jobs being automated, leisure issues, uniqueness loss, AI uses, and accountability.\n",
      "\n",
      "They also pointed out that while ML is great for automation, it can cause serious ethical problems\n",
      "So the question is asking to summarize the key technical aspects and social aspects discussed in these books, specifically focusing on advancements in ML algorithms, ethical concerns, bias in AI, and societal impact of automation.\n",
      "\n",
      "Alright, so I need to cover each of these areas from both the technical and social points\n",
      "Let's start with the technical side.\n",
      "\n",
      "Technical Aspects:\n",
      "1\n",
      "**Algorithms**: They probably talk about different types like supervised, unsupervised, reinforcement learning\n",
      "Maybe mention neural networks since that's a big part now.\n",
      "2\n",
      "**Bias and Variance**: Discuss how ML models can have bias (underfitting) or variance (overfitting), the bias-variance tradeoff is key.\n",
      "3\n",
      "**Ethical Issues in ML**: They'd talk about fairness, accuracy vs\n",
      "justice, the role of data, algorithmic fairness, transparency, etc.\n",
      "\n",
      "Social Aspects:\n",
      "1\n",
      "**Automation Impact**: The book might discuss how automation affects jobs, leisure, uniqueness\n",
      "Like robots making decisions that humans can't see or interact with.\n",
      "2\n",
      "**Ethical Concerns**: There's a section on avoiding unintended consequencesâ€”like if an AI system is biased against someone, it could cause problems.\n",
      "3\n",
      "**Societal Impact**: The books likely talk about the broader effects of ML, like in healthcare, criminal justice, etc., but also the negative side.\n",
      "\n",
      "I should make sure to mention each point from both authors' sections and see how they tie together.\n",
      "\n",
      "Wait, I remember from the ethics book that it's not just about jobs being automated\n",
      "It could lead to automation affecting people's lives more than intended, like in housing, finance, etc\n",
      "So that's an important social aspect where ethical considerations matter beyond just economic outcomes.\n",
      "\n",
      "So putting this all together, I'll structure the answer by first listing each technical and social area, then provide specific points from the books.\n",
      "\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The provided books explore machine learning through both technical advancements and social implications:\n",
      "\n",
      "**Technical Aspects:**\n",
      "1\n",
      "**Algorithms**: The books discuss various ML algorithms, including supervised, unsupervised, and reinforcement learning\n",
      "Notably, neural networks are a major focus.\n",
      "2\n",
      "**Bias and Variance**: They delve into the bias-variance tradeoff, explaining how models can either underfit (high bias) or overfit (high variance), crucial for understanding model performance.\n",
      "3\n",
      "**Ethical Issues in ML**: The ethical concerns include fairness, accuracy vs\n",
      "justice, algorithmic fairness, transparency, and accountability.\n",
      "\n",
      "**Social Aspects:**\n",
      "1\n",
      "**Automation Impact**: Automation affects jobs, leisure, and uniqueness\n",
      "For instance, robots may make decisions humans can't see or interact with.\n",
      "2\n",
      "**Ethical Concerns**: Beyond job automation, ethical issues arise from unintended consequences, such as biased algorithms leading to systemic inequities.\n",
      "3\n",
      "**Societal Impact**: ML has significant effects on society, affecting healthcare, criminal justice, and environmental outcomes, though also introducing potential negative impacts.\n",
      "\n",
      "These books collectively provide insights into both the technical innovations of ML and their societal and ethical implications.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\"Geron2017.pdf\", \"knafling2015.pdf\", \"Mohri2018.pdf\", \"NG2018.pdf\", \"NG2023.pdf\", \"Russell2010.pdf\", \"Zheng2018.pdf\"]\n",
    "query = '''\n",
    "Summarize the key technical and social aspects discussed in the provided books \n",
    "on machine learning. Specifically, highlight advancements in ML algorithms, \n",
    "ethical concerns, bias in AI, and the societal impact of automation.\n",
    "'''\n",
    "\n",
    "response = run_pipeline(pdf_files, query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up conversational retrieval with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_conversational_chain(vectorstore, model_name=\"deepseek-r1:1.5b\"):\n",
    "    \"\"\"Creates a retrieval-based QA system with memory.\"\"\"\n",
    "    llm = Ollama(model=model_name)  \n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=vectorstore.as_retriever(), memory=memory)\n",
    "    return qa_chain\n",
    "\n",
    "def ask_question(qa_chain, query):\n",
    "    \"\"\"Asks a question and stores the conversation history.\"\"\"\n",
    "    response = qa_chain.invoke({\"question\": query})\n",
    "    print(\"ğŸ” Answer:\", response.get(\"answer\", \"No answer found.\"))\n",
    "    return response.get(\"answer\", \"No answer found.\")\n",
    "\n",
    "def run_pipeline_with_memory(pdf_files, query):\n",
    "    documents = load_and_process_pdfs(pdf_files)  \n",
    "    texts = split_text_into_chunks(documents) \n",
    "    \n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    documents = [Document(page_content=text) for text in texts]  \n",
    "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "    \n",
    "    qa_chain = setup_conversational_chain(vectorstore)\n",
    "    \n",
    "    answer = ask_question(qa_chain, query)\n",
    "    answer = answer.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
    "\n",
    "    return answer, qa_chain\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/6_13thh50qqgx2lhw_0qf67m0000gn/T/ipykernel_25020/364125185.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Answer: <think>\n",
      "Okay, so I need to summarize the key technical and social aspects of machine learning books based on the given context. Let's break down what's provided.\n",
      "\n",
      "First, there are a few paragraphs about specific authors and their books. Stuart Russell and Peter Norvig wrote \"Artificial Intelligence: A Modern Approach,\" which is a comprehensive textbook covering a lot of topics including ML. Another author is John Langford from Carnegie Mellon University who has also written \"Revisiting the Bias-Variance Tradeoff.\" There's also a chapter titled \"Machine Learning for Data Science\" in this book, focusing on practical aspects.\n",
      "\n",
      "Then, there's an example about using case studies to explain concepts without getting into specifics. The book is used by many industries and helps leaders understand ML principles through examples from various fields.\n",
      "\n",
      "So the user wants me to list these technical and social aspects. From the context, I can gather:\n",
      "\n",
      "1. Technical Content: The book covers various topics in AI, machine learning, data visualization, bias-variance tradeoff, neural networks, deep learning, natural language processing, automated reasoning, and ethical considerations.\n",
      "2. Practical Tips and Resources: They include case studies, real-world examples, code snippets, detailed explanations of algorithms, and exercises to test understanding.\n",
      "3. Leadership Tools and Tools for Managing Teams: The book provides guidance on defining technical direction and using metrics, along with tips on effective communication.\n",
      "\n",
      "I think that's the main summary based on the provided context.\n",
      "</think>\n",
      "\n",
      "**Summary of Key Technical and Social Aspects in ML Books**\n",
      "\n",
      "1. **Technical Content:**\n",
      "   - Comprehensive coverage of AI and machine learning topics, including data visualization, bias-variance tradeoff, neural networks, deep learning, natural language processing, automated reasoning, ethical considerations, optimization techniques, model evaluation, ensemble methods, feature engineering, reinforcement learning, generative models like GANs, transformers for sequence modeling, transfer learning, domain adaptation, domain generalization, active learning strategies.\n",
      "\n",
      "2. **Practical Tips and Resources:**\n",
      "   - Utilizes case studies across multiple industries to illustrate fundamental concepts without delving into specifics.\n",
      "   - Provides real-world examples and explanations of algorithms with detailed walkthroughs and step-by-step derivations.\n",
      "   - Includes exercises for hands-on practice, covering datasets, model design, implementation details, evaluation metrics, and ethical considerations.\n",
      "\n",
      "3. **Leadership Tools and Team Management:**\n",
      "   - Offers guidance on setting technical direction and defining project scopes.\n",
      "   - Provides leadership tips, such as using single-number metrics when appropriate but emphasizing the importance of diverse perspectives in communication to avoid biases.\n",
      "   - Includes tools and techniques for managing teams effectively within a data-driven context.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\"Geron2017.pdf\", \"knafling2015.pdf\", \"Mohri2018.pdf\", \"NG2018.pdf\", \"NG2023.pdf\", \"Russell2010.pdf\", \"Zheng2018.pdf\"]\n",
    "query = \"Summarize key technical and social aspects in ML books.\"\n",
    "answer, qa_chain = run_pipeline_with_memory(pdf_files, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Answer: <think>\n",
      "å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯MLæ•™æçš„ä¸»è¦æŠ€æœ¯å’Œç¤¾ä¼šå†…å®¹ã€‚æˆ‘éœ€è¦å…ˆç†æ¸…è¿™äº›æ–¹é¢ã€‚\n",
      "\n",
      "é¦–å…ˆï¼ŒæŠ€æœ¯æ–¹é¢åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹ã€ç®—æ³•è®¾è®¡ã€æ¨¡å‹è¯„ä¼°ã€éƒ¨ç½²ä¸ä¼˜åŒ–ã€åº”ç”¨åœºæ™¯ç­‰ã€‚è¿™äº›éƒ½æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒé—®é¢˜å’Œæ–¹æ³•è®ºã€‚\n",
      "\n",
      "ç„¶åæ˜¯ç¤¾äº¤æ–¹é¢ï¼Œæ¶‰åŠä¼¦ç†ã€å¯è§£é‡Šæ€§ã€ç¤¾åŒºå‚ä¸ã€å›¢é˜Ÿåä½œã€å…¬ä¼—è®¤çŸ¥å’Œåé¦ˆæœºåˆ¶ç­‰ã€‚è¿™éƒ¨åˆ†æ˜¯å…³äºå¦‚ä½•åœ¨æŠ€æœ¯ä¸Šå®æ–½MLï¼Œå¹¶è€ƒè™‘äººç±»çš„è§†è§’ã€‚\n",
      "\n",
      "æ€»ç»“ä¸€ä¸‹ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›äº†è§£æ•™æä¸­ç³»ç»Ÿåœ°æ¶µç›–äº†æŠ€æœ¯åŸç†å’ŒæŠ€æœ¯åº”ç”¨ï¼ŒåŒæ—¶ä¹Ÿåœ¨ç¤¾ä¼šå±‚é¢è€ƒè™‘ç›¸å…³é—®é¢˜ã€‚è¿™æ ·å¯ä»¥å…¨é¢å±•ç¤ºæœºå™¨å­¦ä¹ çš„å‘å±•å’Œå½±å“ã€‚\n",
      "</think>\n",
      "\n",
      "åœ¨ ML æ•™æä¸­ï¼Œæœ‰å“ªäº›ä¸»è¦çš„æŠ€æœ¯å’Œç¤¾äº¤æ–¹é¢å†…å®¹ï¼Ÿ\n",
      "\n",
      "ä¸»è¦çš„æŠ€æœ¯æ–¹é¢åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹ã€ç®—æ³•è®¾è®¡ã€æ¨¡å‹è¯„ä¼°ã€éƒ¨ç½²ä¸ä¼˜åŒ–ä»¥åŠåº”ç”¨åœºæ™¯ç­‰ã€‚\n",
      "\n",
      "ä¸»è¦çš„ç¤¾äº¤æ–¹é¢åŒ…æ‹¬ä¼¦ç†ã€å¯è§£é‡Šæ€§ã€ç¤¾åŒºå‚ä¸ã€å›¢é˜Ÿåä½œã€å…¬ä¼—è®¤çŸ¥å’Œåé¦ˆæœºåˆ¶ç­‰ã€‚\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nå—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯MLæ•™æçš„ä¸»è¦æŠ€æœ¯å’Œç¤¾ä¼šå†…å®¹ã€‚æˆ‘éœ€è¦å…ˆç†æ¸…è¿™äº›æ–¹é¢ã€‚\\n\\né¦–å…ˆï¼ŒæŠ€æœ¯æ–¹é¢åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹ã€ç®—æ³•è®¾è®¡ã€æ¨¡å‹è¯„ä¼°ã€éƒ¨ç½²ä¸ä¼˜åŒ–ã€åº”ç”¨åœºæ™¯ç­‰ã€‚è¿™äº›éƒ½æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒé—®é¢˜å’Œæ–¹æ³•è®ºã€‚\\n\\nç„¶åæ˜¯ç¤¾äº¤æ–¹é¢ï¼Œæ¶‰åŠä¼¦ç†ã€å¯è§£é‡Šæ€§ã€ç¤¾åŒºå‚ä¸ã€å›¢é˜Ÿåä½œã€å…¬ä¼—è®¤çŸ¥å’Œåé¦ˆæœºåˆ¶ç­‰ã€‚è¿™éƒ¨åˆ†æ˜¯å…³äºå¦‚ä½•åœ¨æŠ€æœ¯ä¸Šå®æ–½MLï¼Œå¹¶è€ƒè™‘äººç±»çš„è§†è§’ã€‚\\n\\næ€»ç»“ä¸€ä¸‹ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›äº†è§£æ•™æä¸­ç³»ç»Ÿåœ°æ¶µç›–äº†æŠ€æœ¯åŸç†å’ŒæŠ€æœ¯åº”ç”¨ï¼ŒåŒæ—¶ä¹Ÿåœ¨ç¤¾ä¼šå±‚é¢è€ƒè™‘ç›¸å…³é—®é¢˜ã€‚è¿™æ ·å¯ä»¥å…¨é¢å±•ç¤ºæœºå™¨å­¦ä¹ çš„å‘å±•å’Œå½±å“ã€‚\\n</think>\\n\\nåœ¨ ML æ•™æä¸­ï¼Œæœ‰å“ªäº›ä¸»è¦çš„æŠ€æœ¯å’Œç¤¾äº¤æ–¹é¢å†…å®¹ï¼Ÿ\\n\\nä¸»è¦çš„æŠ€æœ¯æ–¹é¢åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€ç‰¹å¾å·¥ç¨‹ã€ç®—æ³•è®¾è®¡ã€æ¨¡å‹è¯„ä¼°ã€éƒ¨ç½²ä¸ä¼˜åŒ–ä»¥åŠåº”ç”¨åœºæ™¯ç­‰ã€‚\\n\\nä¸»è¦çš„ç¤¾äº¤æ–¹é¢åŒ…æ‹¬ä¼¦ç†ã€å¯è§£é‡Šæ€§ã€ç¤¾åŒºå‚ä¸ã€å›¢é˜Ÿåä½œã€å…¬ä¼—è®¤çŸ¥å’Œåé¦ˆæœºåˆ¶ç­‰ã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_query = \"Which of the books focus more on ethical aspects of AI\"\n",
    "ask_question(qa_chain, follow_up_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Answer: <think>\n",
      "å—¯ï¼Œç”¨æˆ·è®©æˆ‘æŠŠä¹‹å‰çš„è‹±æ–‡æé—®é‡è¿°æˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ã€‚æˆ‘éœ€è¦ä»”ç»†åˆ†æä»–ä»¬çš„åŸæ–‡ï¼Œçœ‹çœ‹ä»–ä»¬åˆ°åº•æƒ³è¦ä»€ä¹ˆã€‚\n",
      "\n",
      "ä»–ä»¬æåˆ°äº†ä¸€æœ¬å…³äºæœºå™¨å­¦ä¹ çš„ä¹¦ç±ï¼Œè¯¢é—®å“ªäº›ä¹¦æ›´å¤šåœ°å…³æ³¨ä¼¦ç†æ–¹é¢çš„å†…å®¹ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸­çš„é“å¾·é—®é¢˜ã€ä¼¦ç†å†³ç­–æˆ–è€…æ³•å¾‹å±‚é¢çš„å½±å“ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘è¦ç¡®ä¿æ–°æé—®æ¸…æ™°æ˜ç¡®ï¼Œä¸å¼•å…¥å…¶ä»–ä¿¡æ¯æˆ–å‡è®¾ï¼Œåªä¸“æ³¨äºç”¨æˆ·çš„é—®é¢˜ã€‚æ‰€ä»¥ï¼Œæˆ‘å°†é‡æ–°è¡¨è¿°ä¸ºï¼šâ€œWhich of the ML textbooks focus more on ethical considerations and practices in AIâ€ï¼Ÿ\n",
      "\n",
      "æ˜¯çš„ï¼Œè¿™æ ·æ—¢ä¿æŒäº†åŸæ„ï¼Œåˆç‹¬ç«‹è§£ç­”äº†é—®é¢˜ï¼Œä¸ä¼šæœ‰ä»»ä½•æ··æ·†ã€‚\n",
      "</think>\n",
      "\n",
      "Which of the ML textbooks focus more on ethical considerations and practices in AI?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nå—¯ï¼Œç”¨æˆ·è®©æˆ‘æŠŠä¹‹å‰çš„è‹±æ–‡æé—®é‡è¿°æˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ã€‚æˆ‘éœ€è¦ä»”ç»†åˆ†æä»–ä»¬çš„åŸæ–‡ï¼Œçœ‹çœ‹ä»–ä»¬åˆ°åº•æƒ³è¦ä»€ä¹ˆã€‚\\n\\nä»–ä»¬æåˆ°äº†ä¸€æœ¬å…³äºæœºå™¨å­¦ä¹ çš„ä¹¦ç±ï¼Œè¯¢é—®å“ªäº›ä¹¦æ›´å¤šåœ°å…³æ³¨ä¼¦ç†æ–¹é¢çš„å†…å®¹ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸­çš„é“å¾·é—®é¢˜ã€ä¼¦ç†å†³ç­–æˆ–è€…æ³•å¾‹å±‚é¢çš„å½±å“ã€‚\\n\\næ¥ä¸‹æ¥ï¼Œæˆ‘è¦ç¡®ä¿æ–°æé—®æ¸…æ™°æ˜ç¡®ï¼Œä¸å¼•å…¥å…¶ä»–ä¿¡æ¯æˆ–å‡è®¾ï¼Œåªä¸“æ³¨äºç”¨æˆ·çš„é—®é¢˜ã€‚æ‰€ä»¥ï¼Œæˆ‘å°†é‡æ–°è¡¨è¿°ä¸ºï¼šâ€œWhich of the ML textbooks focus more on ethical considerations and practices in AIâ€ï¼Ÿ\\n\\næ˜¯çš„ï¼Œè¿™æ ·æ—¢ä¿æŒäº†åŸæ„ï¼Œåˆç‹¬ç«‹è§£ç­”äº†é—®é¢˜ï¼Œä¸ä¼šæœ‰ä»»ä½•æ··æ·†ã€‚\\n</think>\\n\\nWhich of the ML textbooks focus more on ethical considerations and practices in AI?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_query = \"Can you please answer my previous question in English?\"\n",
    "ask_question(qa_chain, follow_up_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Another Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "deepseek-ai/deepseek-embedding-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/deepseek-ai/deepseek-embedding-v1/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/utils/hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67b75c7a-457e8f72180388ec159ca09e;66afd59f-6bc4-4011-9087-47f3aebea056)\n\nRepository Not Found for url: https://huggingface.co/deepseek-ai/deepseek-embedding-v1/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 14\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mDeepSeekEmbeddingsTF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [17], line 6\u001b[0m, in \u001b[0;36mDeepSeekEmbeddingsTF.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/deepseek-embedding-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m TFAutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, from_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:881\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 881\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    883\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:713\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    712\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 713\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/utils/hub.py:365\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: deepseek-ai/deepseek-embedding-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "\n",
    "class DeepSeekEmbeddingsTF:\n",
    "    def __init__(self, model_name=\"deepseek-ai/deepseek-embedding-v1\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFAutoModel.from_pretrained(model_name, from_tf=True)\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        inputs = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "        outputs = self.model(**inputs)\n",
    "        return tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy().squeeze()\n",
    "\n",
    "embedding_model = DeepSeekEmbeddingsTF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
