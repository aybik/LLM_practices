{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/Yetiskin2020.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Terms & Conditions of access and use can be found at\n",
      "https://www.tandfonline.com/action/journalInformation?journalCode=hppc20\n",
      "Popular Communication\n",
      "The International Journal of Media and Culture\n",
      "ISSN: (Print) (Online) Journal homepage: https://www.tandfonline.com/loi/hppc20\n",
      "Paratactic commoning: collective knowledge\n",
      "production networking as political struggle\n",
      "Ebru Yetiskin\n",
      "To cite this article: Ebru Yetiskin (2020): Paratactic commoning: collective knowledge production\n",
      "networking as politic\n"
     ]
    }
   ],
   "source": [
    "print(page.page_content[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'iText 4.2.0 by 1T3XT',\n",
       " 'creator': 'Arbortext Advanced Print Publisher 11.0.3433/W Unicode',\n",
       " 'creationdate': '2020-06-25T12:52:06+05:30',\n",
       " 'keywords': 'Commons; commoning; data; control; tactic; paratactic; obfuscation',\n",
       " 'moddate': '2020-06-30T04:01:46-07:00',\n",
       " 'source': 'docs/Yetiskin2020.pdf',\n",
       " 'total_pages': 18,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.blob_loaders import FileSystemBlobLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.youtube.com/watch?v=vtLfCO4IGXY&ab_channel=DiEM25\"\n",
    "save_dir=\"docs/youtube/\"\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url],save_dir),  # fetch from youtube\n",
    "    #FileSystemBlobLoader(save_dir, glob=\"*.m4a\"),   #fetch locally\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nhandbook/titles-for-programmers.md at master ¬∑ basecamp/handbook ¬∑ GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n        Product\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGitHub Copilot\\n        Write better code with AI\\n      \\n\\n\\n\\n\\n\\n\\n\\nSecurity\\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\nActions\\n        Automa'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/titles-for-programmers.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Document Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/splitters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences\n",
    "\n",
    "| Feature                           | `RecursiveCharacterTextSplitter`              | `CharacterTextSplitter`         |\n",
    "|-----------------------------------|--------------------------------|-------------------------|\n",
    "| **Splitting Strategy**           | Tries to break at meaningful places | Splits at a fixed character (e.g., space) |\n",
    "| **Handles Word Boundaries?**      | ‚úÖ Yes                          | ‚ùå No (may cut words)  |\n",
    "| **Performance**                   | Slightly slower but better structured chunks | Fast but less optimal chunks |\n",
    "| **Best for**                      | Long documents with structured text (e.g., articles, books) | Simple text that doesn‚Äôt require structure |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size =26\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator=' '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive splitting details\n",
    "\n",
    "`RecursiveCharacterTextSplitter` is recommended for generic text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's reduce the chunk size a bit and add a period to our separators:\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/NG2023.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(pages)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token splitting\n",
    "\n",
    "We can also split on token count explicity, if we want.\n",
    "\n",
    "This can be useful because LLMs often have context windows designated in tokens.\n",
    "\n",
    "Tokens are often ~4 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"foo bar bazzyfoo\"\n",
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.0 (Macintosh)', 'creationdate': '2022-12-13T16:08:00-05:00', 'moddate': '2022-12-13T16:08:04-05:00', 'trapped': '/False', 'source': 'docs/NG2023.pdf', 'total_pages': 41, 'page': 0, 'page_label': '1'}, page_content='PAGE 1\\nFounder, DeepLearning.')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 18.0 (Macintosh)', 'creationdate': '2022-12-13T16:08:00-05:00', 'moddate': '2022-12-13T16:08:04-05:00', 'trapped': '/False', 'source': 'docs/NG2023.pdf', 'total_pages': 41, 'page': 1, 'page_label': '2'}, page_content='\\nelectricity. It will \\ntransform and')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Adobe PDF Library 17.0',\n",
       " 'creator': 'Adobe InDesign 18.0 (Macintosh)',\n",
       " 'creationdate': '2022-12-13T16:08:00-05:00',\n",
       " 'moddate': '2022-12-13T16:08:04-05:00',\n",
       " 'trapped': '/False',\n",
       " 'source': 'docs/NG2023.pdf',\n",
       " 'total_pages': 41,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Adobe PDF Library 17.0',\n",
       " 'creator': 'Adobe InDesign 18.0 (Macintosh)',\n",
       " 'creationdate': '2022-12-13T16:08:00-05:00',\n",
       " 'moddate': '2022-12-13T16:08:04-05:00',\n",
       " 'trapped': '/False',\n",
       " 'source': 'docs/NG2023.pdf',\n",
       " 'total_pages': 41,\n",
       " 'page': 10,\n",
       " 'page_label': '11'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context aware splitting\n",
    "\n",
    "Chunking aims to keep text with common context together.\n",
    "\n",
    "A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
    "\n",
    "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n\n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'}, page_content='Hi this is Jim  \\nHi this is Joe')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'}, page_content='Hi this is Lance')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstores and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overview.png](images/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/vectorstore.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"docs/Geron2017.pdf\"),\n",
    "    PyPDFLoader(\"docs/Zheng2018.pdf\"),\n",
    "    PyPDFLoader(\"docs/NG2018.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 944\n",
      "First document: page_content='Aur√©lien G√©ron\n",
      "Hands-On  \n",
      "Machine Learning  \n",
      "with Scikit-Learn  \n",
      "& TensorFlow  \n",
      "CONCEPTS, TOOLS, AND TECHNIQUES  \n",
      "TO BUILD INTELLIGENT SYSTEMS\n",
      "\u0000D\u0000o\u0000w\u0000n\u0000l\u0000o\u0000a\u0000d\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000f\u0000i\u0000n\u0000e\u0000l\u0000y\u0000b\u0000o\u0000o\u0000k\u0000 \u0000w\u0000w\u0000w\u0000.\u0000f\u0000i\u0000n\u0000e\u0000l\u0000y\u0000b\u0000o\u0000o\u0000k\u0000.\u0000c\u0000o\u0000m' metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2017-03-10T21:55:34+00:00', 'author': 'Aur√©lien G√©ron', 'moddate': '2017-05-16T09:54:54+08:00', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'trapped': '/False', 'source': 'docs/Geron2017.pdf', 'total_pages': 564, 'page': 0, 'page_label': 'Cover'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total documents loaded: {len(docs)}\")\n",
    "print(f\"First document: {docs[0] if docs else 'No documents loaded'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 1605\n",
      "First Chunk: 978-1-491-96229-9\n",
      "[LSI]\n",
      "Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
      "by Aur√©lien G√©ron\n",
      "Copyright ¬© 2017 A\n",
      "ur√©lien G√©ron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O‚ÄôReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O‚ÄôReilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‚Äê\n",
      "tutional sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Editor: Nicole Tache\n",
      "Production Editor: N\n",
      "icholas Adams\n",
      "Copyeditor: Rachel Monaghan\n",
      "Proofreader: Charles Roumeliotis\n",
      "Indexer: Wendy Catalano\n",
      "Interior Designer: Da\n",
      "vid Futato\n",
      "Cover Designer: Randy Comer\n",
      "Illustrator: Rebecca Demarest\n",
      "March 2017:  First Edition\n",
      "Revision History for the First Edition\n",
      "2017-03-10: First Release\n",
      "See h\n",
      "ttp://oreilly.com/catalog/errata.csp?isbn=9781491962299 for release details.\n",
      "The O‚ÄôReilly logo is a registered trademark of O‚ÄôReilly Media, Inc. Hands-On Machine Learning with\n",
      "Scikit-Learn and TensorFlow, the cover image, and related trade dress are trademarks of O‚ÄôReilly Media,\n",
      "Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Chunks: {len(splits)}\")\n",
    "print(f\"First Chunk: {splits[3].page_content}\")  # Show first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of first element in splits: <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of first element in splits: {type(splits[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/6_13thh50qqgx2lhw_0qf67m0000gn/T/ipykernel_77420/1333021179.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631510802407727"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7702031204123162"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759053971445477"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure `splits` is converted into a list of Document objects\n",
    "documents = [doc for doc in splits if doc.page_content.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid documents: 1605\n",
      "First document content: Aur√©lien G√©ron\n",
      "Hands-On  \n",
      "Machine Learning  \n",
      "with Scikit-Learn  \n",
      "& TensorFlow  \n",
      "CONCEPTS, TOOLS, AND TECHNIQUES  \n",
      "TO BUILD INTELLIGENT SYSTEMS\n",
      "\u0000D\u0000o\u0000w\u0000n\u0000l\u0000o\u0000a\u0000d\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000f\u0000i\u0000n\u0000e\u0000l\u0000y\u0000b\u0000o\u0000o\u0000k\u0000 \u0000w\u0000w\u0000w\u0000\n",
      "First document metadata: {'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2017-03-10T21:55:34+00:00', 'author': 'Aur√©lien G√©ron', 'moddate': '2017-05-16T09:54:54+08:00', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'trapped': '/False', 'source': 'docs/Geron2017.pdf', 'total_pages': 564, 'page': 0, 'page_label': 'Cover'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total valid documents: {len(documents)}\")\n",
    "print(f\"First document content: {documents[0].page_content[:200]}\")  # Print first 200 characters\n",
    "print(f\"First document metadata: {documents[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Surrogate Unicode Characters\n",
    "import unicodedata\n",
    "from langchain.schema import Document\n",
    "\n",
    "def clean_text(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=clean_text(doc.page_content), metadata=doc.metadata)\n",
    "    for doc in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# try persist_directory=None if you get tenant error. It can also work in-memory mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1605\n",
      "[Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 1, 'page_label': '2', 'producer': 'Skia/PDF m71', 'source': 'docs/NG2018.pdf', 'total_pages': 118}, page_content='Machine Learning Yearning is a\\n \\n \\ndeeplearning.ai project.\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n¬© 2018 Andrew Ng. All Rights Reserved.\\n \\n \\n \\n \\nPage 2\\nMachine Learning Yearning-Draft\\nAndrew Ng'), Document(metadata={'author': 'Aur√©lien G√©ron', 'creationdate': '2017-03-10T21:55:34+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'moddate': '2017-05-16T09:54:54+08:00', 'page': 25, 'page_label': '4', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': 'docs/Geron2017.pdf', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'total_pages': 564, 'trapped': '/False'}, page_content='If you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, tr\\ny to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data.\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n‚ÄîArthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n‚ÄîTom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called ‚Äúham‚Äù) emails. The examples that the system uses to learn are\\ncalled the training set. Each training example is called a training instance (or sample).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata, and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is'), Document(metadata={'author': 'Aur√©lien G√©ron', 'creationdate': '2017-03-10T21:55:34+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'moddate': '2017-05-16T09:54:54+08:00', 'page': 14, 'page_label': 'xiii', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': 'docs/Geron2017.pdf', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'total_pages': 564, 'trapped': '/False'}, page_content='videos, beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‚Äê\\nognize faces? Or learn to walk around?\\nxiii\\n\\x00D\\x00o\\x00w\\x00n\\x00l\\x00o\\x00a\\x00d\\x00 \\x00f\\x00r\\x00o\\x00m\\x00 \\x00f\\x00i\\x00n\\x00e\\x00l\\x00y\\x00b\\x00o\\x00o\\x00k\\x00 \\x00w\\x00w\\x00w\\x00.\\x00f\\x00i\\x00n\\x00e\\x00l\\x00y\\x00b\\x00o\\x00o\\x00k\\x00.\\x00c\\x00o\\x00m')]\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())  # Number of stored embeddings\n",
    "print(vectordb.similarity_search(\"What is AI?\", k=3))  # Test retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'creationdate': '', 'creator': 'PyPDF', 'page': 91, 'page_label': '92', 'producer': 'Skia/PDF m71', 'source': 'docs/NG2018.pdf', 'total_pages': 118}, page_content='Neural networks are commonly used in end-to-end learning systems. The term ‚Äúend-to-end‚Äù\\n \\nrefers to the fact that we are asking the learning algorithm to go directly from the input to\\n \\nthe desired output. I.e., the learning algorithm directly connects the ‚Äúinput end‚Äù of the\\n \\nsystem to the ‚Äúoutput end.‚Äù\\n \\nIn problems where data is abundant, end-to-end systems have been remarkably successful.\\n \\nBut they are not always a good choice. The next few chapters will give more examples of\\n \\nend-to-end systems as well as give advice on when you should and should not use them.\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 92\\nMachine Learning Yearning-Draft\\nAndrew Ng'), Document(metadata={'author': 'Aur√©lien G√©ron', 'creationdate': '2017-03-10T21:55:34+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'moddate': '2017-05-16T09:54:54+08:00', 'page': 277, 'page_label': '256', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': 'docs/Geron2017.pdf', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'total_pages': 564, 'trapped': '/False'}, page_content='4 In the context of Machine Learning, the phrase ‚Äúneural networks‚Äù generally refers to ANNs, not BNNs.\\n5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.w ikipe\\ndia.org/wiki/Cerebral_cortex.\\nworks (BNN)4 is still the subject of active research, but some parts of the brain ha ve\\nbeen mapped, and it seems tha\\nt neurons are often organized in consecutive layers, as \\nshown in Figure 10-2.\\nFigure 10-2. Multiple layers in a biological neural network (human cortex)5\\nLogical Computations with Neurons\\nWarren McCulloch and W\\nalter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial neuron: it has one or more binar y\\n(on/off) in\\nputs and one binary output. The artificial neuron simply activates its out‚Äê\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let‚Äôs\\nbuild a few ANNs that perform various logical computations (see Figure 10-3),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n256 | Chapter 10: Introduction to Artificial Neural Networks\\n\\x00D\\x00o\\x00w\\x00n\\x00l\\x00o\\x00a\\x00d\\x00 \\x00f\\x00r\\x00o\\x00m\\x00 \\x00f\\x00i\\x00n\\x00e\\x00l\\x00y\\x00b\\x00o\\x00o\\x00k\\x00 \\x00w\\x00w\\x00w\\x00.\\x00f\\x00i\\x00n\\x00e\\x00l\\x00y\\x00b\\x00o\\x00o\\x00k\\x00.\\x00c\\x00o\\x00m'), Document(metadata={'author': 'Aur√©lien G√©ron', 'creationdate': '2017-03-10T21:55:34+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'moddate': '2017-05-16T09:54:54+08:00', 'page': 374, 'page_label': '353', 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': 'docs/Geron2017.pdf', 'title': 'Hands-On Machine Learning with Scikit-Learn and TensorFlow', 'total_pages': 564, 'trapped': '/False'}, page_content='CHAPTER 13\\nConvolutional Neural Networks\\nAlthough IBM‚Äôs Deep Blue supercom\\nputer beat the chess world champion Garry Kas‚Äê\\nparov back in 1996, until quite recently computers were unable to reliably perform\\nseemingly trivial tasks such as detecting a puppy in a picture or recognizing spoken\\nwords. Why are these tasks so effortless to us humans? The answer lies in the fact that\\nperception largely takes place outside the realm of our consciousness, within special‚Äê\\nized visual, auditory, and other sensory modules in our brains. By the time sensory\\ninformation reaches our consciousness, it is already adorned with high-level features;\\nfor example, when you look at a picture of a cute puppy, you cannot choose not to see\\nthe puppy, or not to notice its cuteness. Nor can you explain how you recognize a cute\\npuppy; it‚Äôs just obvious to you. Thus, we cannot trust our subjective experience: per‚Äê\\nception is not trivial at all, and to understand it we must look at how the sensory\\nmodules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain‚Äôs visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‚Äê\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and')]\n"
     ]
    }
   ],
   "source": [
    "result= vectordb.similarity_search(\"What are neural networks?\", k=3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural networks are commonly used in end-to-end learning systems. The term ‚Äúend-to-end‚Äù\\n \\nrefers to the fact that we are asking the learning algorithm to go directly from the input to\\n \\nthe desired output. I.e., the learning algorithm directly connects the ‚Äúinput end‚Äù of the\\n \\nsystem to the ‚Äúoutput end.‚Äù\\n \\nIn problems where data is abundant, end-to-end systems have been remarkably successful.\\n \\nBut they are not always a good choice. The next few chapters will give more examples of\\n \\nend-to-end systems as well as give advice on when you should and should not use them.\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPage 92\\nMachine Learning Yearning-Draft\\nAndrew Ng'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Result 1:**\n",
      "üìÑ Source: docs/NG2018.pdf, Page: 91\n",
      "üìù Content:\n",
      "Neural networks are commonly used in end-to-end learning systems. The term ‚Äúend-\n",
      "to-end‚Äù   refers to the fact that we are asking the learning algorithm to go\n",
      "directly from the input to   the desired output. I.e., the learning algorithm\n",
      "directly connects the ‚Äúinput end‚Äù of the   system to the ‚Äúoutput end.‚Äù   In\n",
      "problems where data is abundant, end-to-end systems have been remarkably\n",
      "successful.   But they are not always a good choice. The next few chapters will\n",
      "give more examples of   end-to-end systems as well as give advice on when you\n",
      "should and should not use them.                       Page 92 Machine Learning\n",
      "Yearning-Draft Andrew Ng\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üîπ **Result 2:**\n",
      "üìÑ Source: docs/Geron2017.pdf, Page: 277\n",
      "üìù Content:\n",
      "4 In the context of Machine Learning, the phrase ‚Äúneural networks‚Äù generally\n",
      "refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal\n",
      "(public domain). Reproduced from https://en.w ikipe\n",
      "dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active\n",
      "research, but some parts of the brain ha ve been mapped, and it seems tha t\n",
      "neurons are often organized in consecutive layers, as  shown in Figure 10-2.\n",
      "Figure 10-2. Multiple layers in a biological neural network (human cortex)5\n",
      "Logical Computations with Neurons Warren McCulloch and W alter Pitts proposed a\n",
      "very simple model of the biological neuron, which later became known as an\n",
      "artificial neuron: it has one or more binar y (on/off) in puts and one binary\n",
      "output. The artificial neuron simply activates its out‚Äê put when more than a\n",
      "certain number of its inputs are active. McCulloch and Pitts showed that even\n",
      "with such a simplified model it is possible to build a network of artificial\n",
      "neurons that computes any logical proposition you want. For example, let‚Äôs build\n",
      "a few ANNs that perform various logical computations (see Figure 10-3), assuming\n",
      "that a neuron is activated when at least two of its inputs are active. Figure\n",
      "10-3. ANNs performing simple logical computations 256 | Chapter 10: Introduction\n",
      "to Artificial Neural Networks \u0000D\u0000o\u0000w\u0000n\u0000l\u0000o\u0000a\u0000d\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000f\u0000i\u0000n\u0000e\u0000l\u0000y\u0000b\u0000o\u0000o\u0000k\u0000\n",
      "\u0000w\u0000w\u0000w\u0000.\u0000f\u0000i\u0000n\u0000e\u0000l\u0000y\u0000b\u0000o\u0000o\u0000k\u0000.\u0000c\u0000o\u0000m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üîπ **Result 3:**\n",
      "üìÑ Source: docs/Geron2017.pdf, Page: 374\n",
      "üìù Content:\n",
      "CHAPTER 13 Convolutional Neural Networks Although IBM‚Äôs Deep Blue supercom puter\n",
      "beat the chess world champion Garry Kas‚Äê parov back in 1996, until quite\n",
      "recently computers were unable to reliably perform seemingly trivial tasks such\n",
      "as detecting a puppy in a picture or recognizing spoken words. Why are these\n",
      "tasks so effortless to us humans? The answer lies in the fact that perception\n",
      "largely takes place outside the realm of our consciousness, within special‚Äê ized\n",
      "visual, auditory, and other sensory modules in our brains. By the time sensory\n",
      "information reaches our consciousness, it is already adorned with high-level\n",
      "features; for example, when you look at a picture of a cute puppy, you cannot\n",
      "choose not to see the puppy, or not to notice its cuteness. Nor can you explain\n",
      "how you recognize a cute puppy; it‚Äôs just obvious to you. Thus, we cannot trust\n",
      "our subjective experience: per‚Äê ception is not trivial at all, and to understand\n",
      "it we must look at how the sensory modules work. Convolutional neural networks\n",
      "(CNNs) emerged from the study of the brain‚Äôs visual cortex, and they have been\n",
      "used in image recognition since the 1980s. In the last few years, thanks to the\n",
      "increase in computational power, the amount of available training data, and the\n",
      "tricks presented in Chapter 11 for training deep nets, CNNs have man‚Äê aged to\n",
      "achieve superhuman performance on some complex visual tasks. They power image\n",
      "search services, self-driving cars, automatic video classification systems, and\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "\n",
    "# Print formatted results\n",
    "for i, doc in enumerate(result):\n",
    "    print(f\"\\nüîπ **Result {i+1}:**\")\n",
    "    print(f\"üìÑ Source: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(\"üìù Content:\")\n",
    "    print(textwrap.fill(doc.page_content, width=80))  # Wrap text to 80 characters\n",
    "    print(\"-\" * 100)  # Divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
